# FaceForensics++ íŒŒì¸íŠœë‹ ì™„ì „ ê°€ì´ë“œ (A to Z)

> ì²˜ìŒë¶€í„° ëê¹Œì§€ ë‹¨ê³„ë³„ ìƒì„¸ ê°€ì´ë“œ

---

## ğŸ“‹ ëª©ì°¨

1. [ê°œìš”](#1-ê°œìš”)
2. [ë°ì´í„°ì…‹ ì‹ ì²­ ë° ë‹¤ìš´ë¡œë“œ](#2-ë°ì´í„°ì…‹-ì‹ ì²­-ë°-ë‹¤ìš´ë¡œë“œ)
3. [í™˜ê²½ ì„¤ì •](#3-í™˜ê²½-ì„¤ì •)
4. [ë°ì´í„° ì „ì²˜ë¦¬](#4-ë°ì´í„°-ì „ì²˜ë¦¬)
5. [í•™ìŠµ ì½”ë“œ ì‘ì„±](#5-í•™ìŠµ-ì½”ë“œ-ì‘ì„±)
6. [íŒŒì¸íŠœë‹ ì‹¤í–‰](#6-íŒŒì¸íŠœë‹-ì‹¤í–‰)
7. [ê²€ì¦ ë° í‰ê°€](#7-ê²€ì¦-ë°-í‰ê°€)
8. [ëª¨ë¸ ì œì¶œ](#8-ëª¨ë¸-ì œì¶œ)
9. [ë¬¸ì œ í•´ê²°](#9-ë¬¸ì œ-í•´ê²°)

---

## 1. ê°œìš”

### ëª©í‘œ
ë² ì´ìŠ¤ë¼ì¸ ViT ëª¨ë¸ì„ FaceForensics++ ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ

### ì˜ˆìƒ íš¨ê³¼
- F1 Score: +5~10%
- ë² ì´ìŠ¤ë¼ì¸ 0.5489 â†’ 0.60~0.65 ëª©í‘œ

### ì†Œìš” ì‹œê°„
- ë°ì´í„° ì¤€ë¹„: 1~2ì¼
- íŒŒì¸íŠœë‹: 4~8ì‹œê°„ (GPU)
- ê²€ì¦ ë° ì œì¶œ: ë°˜ë‚˜ì ˆ
- **ì´: 3~5ì¼**

---

## 2. ë°ì´í„°ì…‹ ì‹ ì²­ ë° ë‹¤ìš´ë¡œë“œ

### Step 2.1: ë°ì´í„°ì…‹ ì‹ ì²­

#### ë°©ë²• A: ê³µì‹ GitHub (ì¶”ì²œ)

```bash
# 1. FaceForensics++ GitHub ë°©ë¬¸
https://github.com/ondyari/FaceForensics

# 2. ì‹ ì²­ ì–‘ì‹ ì‘ì„±
https://github.com/ondyari/FaceForensics/blob/master/dataset/README.md

í•„ìˆ˜ ì •ë³´:
- ì´ë¦„
- ì´ë©”ì¼
- ì†Œì† ê¸°ê´€ (í•™ìƒ/ì—°êµ¬ì›)
- ì‚¬ìš© ëª©ì : "Academic research for deepfake detection competition"
```

#### ìŠ¹ì¸ ì‹œê°„
- ë³´í†µ 1~3ì¼ (ì˜ì—…ì¼ ê¸°ì¤€)
- ìŠ¹ì¸ í›„ ë‹¤ìš´ë¡œë“œ ë§í¬ ì´ë©”ì¼ ìˆ˜ì‹ 

---

### Step 2.2: ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ

#### ì¶”ì²œ: c23 ë²„ì „ (ì••ì¶•, ê²½ëŸ‰)

```bash
# ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ (ìŠ¹ì¸ í›„ ì œê³µ)
python download-FaceForensics.py \
    --dataset FaceForensics++ \
    --compression c23 \
    --type videos \
    --num_videos 1000

# ì˜ˆìƒ ìš©ëŸ‰: 5~10GB
# ì˜ˆìƒ ì‹œê°„: 1~3ì‹œê°„ (ì¸í„°ë„· ì†ë„ì— ë”°ë¼)
```

#### ë°ì´í„°ì…‹ êµ¬ì¡°

```
FaceForensics++/
â”œâ”€â”€ original_sequences/
â”‚   â””â”€â”€ youtube/
â”‚       â””â”€â”€ c23/
â”‚           â””â”€â”€ videos/          # Real ë¹„ë””ì˜¤ (1,000ê°œ)
â”‚
â”œâ”€â”€ manipulated_sequences/
â”‚   â”œâ”€â”€ Deepfakes/
â”‚   â”‚   â””â”€â”€ c23/
â”‚   â”‚       â””â”€â”€ videos/          # Deepfakes (1,000ê°œ)
â”‚   â”œâ”€â”€ Face2Face/
â”‚   â”‚   â””â”€â”€ c23/
â”‚   â”‚       â””â”€â”€ videos/          # Face2Face (1,000ê°œ)
â”‚   â”œâ”€â”€ FaceSwap/
â”‚   â”‚   â””â”€â”€ c23/
â”‚   â”‚       â””â”€â”€ videos/          # FaceSwap (1,000ê°œ)
â”‚   â””â”€â”€ NeuralTextures/
â”‚       â””â”€â”€ c23/
â”‚           â””â”€â”€ videos/          # NeuralTextures (1,000ê°œ)
```

#### ë‹¤ìš´ë¡œë“œ ìœ„ì¹˜

```bash
# í”„ë¡œì íŠ¸ ì™¸ë¶€ì— ì €ì¥ (ìš©ëŸ‰ ë•Œë¬¸)
C:\Datasets\FaceForensics++\

# ë˜ëŠ”
D:\Data\FaceForensics++\
```

---

## 3. í™˜ê²½ ì„¤ì •

### Step 3.1: GPU í™•ì¸

```bash
# CUDA ì„¤ì¹˜ í™•ì¸
nvidia-smi

# PyTorch GPU ì‚¬ìš© ê°€ëŠ¥ í™•ì¸
python -c "import torch; print(torch.cuda.is_available())"
```

### Step 3.2: í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
cd C:\Users\jinse\Documents\GitHub\deepfake
venv\Scripts\activate

# ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install -U transformers==4.30
pip install -U datasets
pip install -U scikit-learn
pip install -U tqdm
pip install -U tensorboard  # í•™ìŠµ ëª¨ë‹ˆí„°ë§ìš©
```

---

## 4. ë°ì´í„° ì „ì²˜ë¦¬

### Step 4.1: ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±

```python
# scripts/preprocess_ff++.py
import os
import cv2
import dlib
import numpy as np
from PIL import Image
from pathlib import Path
from tqdm import tqdm
import json

# ì„¤ì •
FF_ROOT = "C:/Datasets/FaceForensics++"  # ë‹¤ìš´ë¡œë“œ ìœ„ì¹˜
OUTPUT_ROOT = "./data/ff++_processed"
NUM_FRAMES = 10  # ë¹„ë””ì˜¤ë‹¹ í”„ë ˆì„ ìˆ˜
TARGET_SIZE = (224, 224)
MARGIN = 1.3

# ë² ì´ìŠ¤ë¼ì¸ê³¼ ë™ì¼í•œ ì–¼êµ´ ê²€ì¶œ í•¨ìˆ˜
def get_boundingbox(face, width, height, margin=MARGIN):
    x1, y1, x2, y2 = face.left(), face.top(), face.right(), face.bottom()
    size_bb = int(max(x2 - x1, y2 - y1) * margin)
    center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2
    x1 = max(int(center_x - size_bb // 2), 0)
    y1 = max(int(center_y - size_bb // 2), 0)
    size_bb = min(width - x1, size_bb)
    size_bb = min(height - y1, size_bb)
    return x1, y1, size_bb

def extract_face(image, detector):
    """ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ ì¶”ì¶œ"""
    if isinstance(image, np.ndarray):
        img_np = image
    else:
        img_np = np.array(image)
    
    height, width = img_np.shape[:2]
    
    # ê²€ì¶œìš© ë¦¬ì‚¬ì´ì¦ˆ
    if width > 640:
        scale = 640 / float(width)
        resized_h = int(height * scale)
        resized = cv2.resize(img_np, (640, resized_h))
    else:
        scale = 1.0
        resized = img_np
    
    # ì–¼êµ´ ê²€ì¶œ
    faces = detector(resized, 1)
    if not faces:
        return None
    
    # ê°€ì¥ í° ì–¼êµ´
    face = max(faces, key=lambda r: r.width() * r.height())
    
    # ìŠ¤ì¼€ì¼ ì¡°ì •
    face_rect = dlib.rectangle(
        left=int(face.left() / scale),
        top=int(face.top() / scale),
        right=int(face.right() / scale),
        bottom=int(face.bottom() / scale)
    )
    
    # í¬ë¡­
    x, y, size = get_boundingbox(face_rect, width, height)
    cropped = img_np[y:y+size, x:x+size]
    
    # ë¦¬ì‚¬ì´ì¦ˆ
    face_img = Image.fromarray(cropped).resize(TARGET_SIZE, Image.BICUBIC)
    return face_img

def process_video(video_path, label, detector, output_dir):
    """ë¹„ë””ì˜¤ì—ì„œ í”„ë ˆì„ ì¶”ì¶œ ë° ì–¼êµ´ í¬ë¡­"""
    cap = cv2.VideoCapture(str(video_path))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    if total_frames <= 0:
        cap.release()
        return 0
    
    # ê· ë“± ìƒ˜í”Œë§
    frame_indices = np.linspace(0, total_frames-1, NUM_FRAMES, dtype=int)
    
    saved_count = 0
    video_name = video_path.stem
    
    for i, idx in enumerate(frame_indices):
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        
        if not ret:
            continue
        
        # BGR to RGB
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # ì–¼êµ´ ì¶”ì¶œ
        face_img = extract_face(frame_rgb, detector)
        
        if face_img is None:
            continue
        
        # ì €ì¥
        save_path = output_dir / f"{video_name}_frame{i:03d}.jpg"
        face_img.save(save_path, quality=95)
        saved_count += 1
    
    cap.release()
    return saved_count

def main():
    print("FaceForensics++ ì „ì²˜ë¦¬ ì‹œì‘")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_train = Path(OUTPUT_ROOT) / "train"
    output_val = Path(OUTPUT_ROOT) / "val"
    
    for split in [output_train, output_val]:
        (split / "real").mkdir(parents=True, exist_ok=True)
        (split / "fake").mkdir(parents=True, exist_ok=True)
    
    # dlib ê²€ì¶œê¸°
    detector = dlib.get_frontal_face_detector()
    
    # ë°ì´í„°ì…‹ ê²½ë¡œ
    real_path = Path(FF_ROOT) / "original_sequences/youtube/c23/videos"
    fake_paths = [
        Path(FF_ROOT) / "manipulated_sequences/Deepfakes/c23/videos",
        Path(FF_ROOT) / "manipulated_sequences/Face2Face/c23/videos",
        Path(FF_ROOT) / "manipulated_sequences/FaceSwap/c23/videos",
        Path(FF_ROOT) / "manipulated_sequences/NeuralTextures/c23/videos",
    ]
    
    # Real ë¹„ë””ì˜¤ ì²˜ë¦¬
    print("\nReal ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘...")
    real_videos = list(real_path.glob("*.mp4"))
    
    # Train/Val ë¶„í•  (80/20)
    split_idx = int(len(real_videos) * 0.8)
    train_real = real_videos[:split_idx]
    val_real = real_videos[split_idx:]
    
    for videos, output_dir in [(train_real, output_train / "real"), 
                                (val_real, output_val / "real")]:
        for video in tqdm(videos, desc=f"Real ({output_dir.parent.name})"):
            process_video(video, 0, detector, output_dir)
    
    # Fake ë¹„ë””ì˜¤ ì²˜ë¦¬
    print("\nFake ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘...")
    all_fake_videos = []
    for fake_path in fake_paths:
        all_fake_videos.extend(list(fake_path.glob("*.mp4")))
    
    # Train/Val ë¶„í• 
    split_idx = int(len(all_fake_videos) * 0.8)
    train_fake = all_fake_videos[:split_idx]
    val_fake = all_fake_videos[split_idx:]
    
    for videos, output_dir in [(train_fake, output_train / "fake"), 
                                (val_fake, output_val / "fake")]:
        for video in tqdm(videos, desc=f"Fake ({output_dir.parent.name})"):
            process_video(video, 1, detector, output_dir)
    
    # í†µê³„
    print("\nì „ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"Train Real: {len(list((output_train/'real').glob('*.jpg')))}")
    print(f"Train Fake: {len(list((output_train/'fake').glob('*.jpg')))}")
    print(f"Val Real: {len(list((output_val/'real').glob('*.jpg')))}")
    print(f"Val Fake: {len(list((output_val/'fake').glob('*.jpg')))}")
    print(f"\nì €ì¥ ìœ„ì¹˜: {OUTPUT_ROOT}")

if __name__ == "__main__":
    main()
```

### Step 4.2: ì „ì²˜ë¦¬ ì‹¤í–‰

```bash
# ì „ì²˜ë¦¬ ì‹œì‘ (ì‹œê°„ ì†Œìš”: 2~4ì‹œê°„)
python scripts/preprocess_ff++.py

# ì˜ˆìƒ ê²°ê³¼:
# data/ff++_processed/
# â”œâ”€â”€ train/
# â”‚   â”œâ”€â”€ real/     (~8,000 ì´ë¯¸ì§€)
# â”‚   â””â”€â”€ fake/     (~32,000 ì´ë¯¸ì§€)
# â””â”€â”€ val/
#     â”œâ”€â”€ real/     (~2,000 ì´ë¯¸ì§€)
#     â””â”€â”€ fake/     (~8,000 ì´ë¯¸ì§€)
```

---

## 5. í•™ìŠµ ì½”ë“œ ì‘ì„±

### Step 5.1: í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

```python
# scripts/finetune_vit.py
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import ViTForImageClassification, ViTImageProcessor
from PIL import Image
from pathlib import Path
from tqdm import tqdm
import numpy as np
from sklearn.metrics import f1_score, accuracy_score
import json

# ì„¤ì •
BASELINE_MODEL = "./baseline/model/deep-fake-detector-v2-model"
DATA_ROOT = "./data/ff++_processed"
OUTPUT_DIR = "./models/vit_finetuned"
BATCH_SIZE = 16
LEARNING_RATE = 1e-5
EPOCHS = 5
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ë°ì´í„°ì…‹ í´ë˜ìŠ¤
class DeepfakeDataset(Dataset):
    def __init__(self, root, split='train', processor=None):
        self.root = Path(root) / split
        self.processor = processor
        
        # íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
        self.samples = []
        
        # Real (label=0)
        real_dir = self.root / "real"
        for img_path in real_dir.glob("*.jpg"):
            self.samples.append((str(img_path), 0))
        
        # Fake (label=1)
        fake_dir = self.root / "fake"
        for img_path in fake_dir.glob("*.jpg"):
            self.samples.append((str(img_path), 1))
        
        print(f"{split} dataset: {len(self.samples)} samples")
        print(f"  Real: {len(list(real_dir.glob('*.jpg')))}")
        print(f"  Fake: {len(list(fake_dir.glob('*.jpg')))}")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = Image.open(img_path).convert('RGB')
        
        # ì „ì²˜ë¦¬
        if self.processor:
            inputs = self.processor(images=image, return_tensors="pt")
            pixel_values = inputs['pixel_values'].squeeze(0)
        else:
            pixel_values = torch.zeros((3, 224, 224))
        
        return pixel_values, label

# í•™ìŠµ í•¨ìˆ˜
def train_epoch(model, dataloader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    all_preds = []
    all_labels = []
    
    for images, labels in tqdm(dataloader, desc="Training"):
        images = images.to(device)
        labels = labels.to(device)
        
        # Forward
        outputs = model(pixel_values=images)
        logits = outputs.logits
        loss = criterion(logits, labels)
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # í†µê³„
        total_loss += loss.item()
        preds = torch.argmax(logits, dim=1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
    
    avg_loss = total_loss / len(dataloader)
    acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average='macro')
    
    return avg_loss, acc, f1

# ê²€ì¦ í•¨ìˆ˜
def validate(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for images, labels in tqdm(dataloader, desc="Validating"):
            images = images.to(device)
            labels = labels.to(device)
            
            outputs = model(pixel_values=images)
            logits = outputs.logits
            loss = criterion(logits, labels)
            
            total_loss += loss.item()
            preds = torch.argmax(logits, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    avg_loss = total_loss / len(dataloader)
    acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average='macro')
    
    return avg_loss, acc, f1

# ë©”ì¸ í•™ìŠµ ë£¨í”„
def main():
    print("="*60)
    print("FaceForensics++ íŒŒì¸íŠœë‹ ì‹œì‘")
    print("="*60)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)
    
    # ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ
    print("\në² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ ë¡œë“œ...")
    model = ViTForImageClassification.from_pretrained(BASELINE_MODEL)
    processor = ViTImageProcessor.from_pretrained(BASELINE_MODEL)
    model = model.to(DEVICE)
    print(f"Device: {DEVICE}")
    
    # ë°ì´í„°ì…‹
    print("\në°ì´í„°ì…‹ ì¤€ë¹„...")
    train_dataset = DeepfakeDataset(DATA_ROOT, 'train', processor)
    val_dataset = DeepfakeDataset(DATA_ROOT, 'val', processor)
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=True, 
        num_workers=4
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=False, 
        num_workers=4
    )
    
    # ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤ í•¨ìˆ˜
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.CrossEntropyLoss()
    
    # í•™ìŠµ
    best_f1 = 0
    history = []
    
    print("\ní•™ìŠµ ì‹œì‘!")
    print("="*60)
    
    for epoch in range(EPOCHS):
        print(f"\nEpoch {epoch+1}/{EPOCHS}")
        print("-"*60)
        
        # Train
        train_loss, train_acc, train_f1 = train_epoch(
            model, train_loader, optimizer, criterion, DEVICE
        )
        
        # Validate
        val_loss, val_acc, val_f1 = validate(
            model, val_loader, criterion, DEVICE
        )
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nTrain Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}")
        print(f"Val   Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}")
        
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        history.append({
            'epoch': epoch + 1,
            'train_loss': train_loss,
            'train_acc': train_acc,
            'train_f1': train_f1,
            'val_loss': val_loss,
            'val_acc': val_acc,
            'val_f1': val_f1
        })
        
        # ìµœê³  ëª¨ë¸ ì €ì¥
        if val_f1 > best_f1:
            best_f1 = val_f1
            save_path = Path(OUTPUT_DIR) / "best_model"
            model.save_pretrained(save_path)
            processor.save_pretrained(save_path)
            print(f"âœ“ Best model saved! (F1: {best_f1:.4f})")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_path = Path(OUTPUT_DIR) / "final_model"
    model.save_pretrained(final_path)
    processor.save_pretrained(final_path)
    
    # íˆìŠ¤í† ë¦¬ ì €ì¥
    with open(Path(OUTPUT_DIR) / "history.json", 'w') as f:
        json.dump(history, f, indent=2)
    
    print("\n" + "="*60)
    print("í•™ìŠµ ì™„ë£Œ!")
    print(f"Best Val F1: {best_f1:.4f}")
    print(f"ëª¨ë¸ ì €ì¥: {OUTPUT_DIR}")
    print("="*60)

if __name__ == "__main__":
    main()
```

---

## 6. íŒŒì¸íŠœë‹ ì‹¤í–‰

### Step 6.1: í•™ìŠµ ì‹œì‘

```bash
# ì „ì²˜ë¦¬ ì™„ë£Œ í™•ì¸
ls data/ff++_processed/train/

# í•™ìŠµ ì‹œì‘ (4~8ì‹œê°„ ì†Œìš”)
python scripts/finetune_vit.py

# ì˜ˆìƒ ì¶œë ¥:
# ============================================================
# FaceForensics++ íŒŒì¸íŠœë‹ ì‹œì‘
# ============================================================
# 
# ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ ë¡œë“œ...
# Device: cuda
# 
# ë°ì´í„°ì…‹ ì¤€ë¹„...
# train dataset: 40000 samples
#   Real: 8000
#   Fake: 32000
# val dataset: 10000 samples
#   Real: 2000
#   Fake: 8000
# 
# í•™ìŠµ ì‹œì‘!
# ============================================================
# 
# Epoch 1/5
# ------------------------------------------------------------
# Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [12:34<00:00,  3.31it/s]
# Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|  625/625 [01:23<00:00,  7.49it/s]
# 
# Train Loss: 0.2345 | Acc: 0.9123 | F1: 0.9056
# Val   Loss: 0.3456 | Acc: 0.8876 | F1: 0.8834
# âœ“ Best model saved! (F1: 0.8834)
```

### Step 6.2: í•™ìŠµ ëª¨ë‹ˆí„°ë§

```python
# í•™ìŠµ ì¤‘ ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ í™•ì¸
import json
with open('./models/vit_finetuned/history.json') as f:
    history = json.load(f)
    
for h in history:
    print(f"Epoch {h['epoch']}: Val F1 = {h['val_f1']:.4f}")
```

---

## 7. ê²€ì¦ ë° í‰ê°€

### Step 7.1: ë¡œì»¬ ê²€ì¦

```python
# scripts/evaluate_model.py
from transformers import ViTForImageClassification, ViTImageProcessor
from PIL import Image
import torch

# íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ
model_path = "./models/vit_finetuned/best_model"
model = ViTForImageClassification.from_pretrained(model_path).to("cuda")
processor = ViTImageProcessor.from_pretrained(model_path)
model.eval()

# ìƒ˜í”Œ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸
test_image = Image.open("samples/fake/image/sample_image_1.png")

inputs = processor(images=test_image, return_tensors="pt").to("cuda")
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits
    pred = torch.argmax(logits, dim=1).item()

print(f"Prediction: {pred} ({'Fake' if pred == 1 else 'Real'})")
```

---

## 8. ëª¨ë¸ ì œì¶œ

### Step 8.1: ì œì¶œ ì¤€ë¹„

```bash
# íŒŒì¸íŠœë‹ ëª¨ë¸ì„ submit í´ë”ë¡œ ë³µì‚¬
mkdir -p submit/model_finetuned
cp -r models/vit_finetuned/best_model/* submit/model_finetuned/

# ë˜ëŠ” baseline ëª¨ë¸ ëŒ€ì²´
rm -rf submit/model/deep-fake-detector-v2-model/*
cp -r models/vit_finetuned/best_model/* submit/model/deep-fake-detector-v2-model/
```

### Step 8.2: task.ipynb ìˆ˜ì •

```python
# submit/task.ipynbì—ì„œ ëª¨ë¸ ê²½ë¡œ í™•ì¸
model_path = "./model/deep-fake-detector-v2-model"  # íŒŒì¸íŠœë‹ ëª¨ë¸ í¬í•¨

# ë‚˜ë¨¸ì§€ëŠ” ë™ì¼ (EXP-003 TTA ì½”ë“œ)
```

### Step 8.3: ì œì¶œ

```python
# submit/task.ipynb Cell 19
import aifactory.score as aif

aif.submit(
    model_name="EXP-004_finetuned",
    key="your_key_here"
)
```

---

## 9. ë¬¸ì œ í•´ê²°

### Q1: GPU ë©”ëª¨ë¦¬ ë¶€ì¡± (OOM)

```python
# finetune_vit.pyì—ì„œ ë°°ì¹˜ í¬ê¸° ê°ì†Œ
BATCH_SIZE = 16  # â†’ 8 ë˜ëŠ” 4
```

### Q2: ë°ì´í„° ë¶ˆê· í˜• (Real < Fake)

```python
# ê°€ì¤‘ì¹˜ ì†ì‹¤ í•¨ìˆ˜ ì‚¬ìš©
from torch.nn import CrossEntropyLoss

# Real:Fake = 1:4 ë¹„ìœ¨
weight = torch.tensor([4.0, 1.0]).to(DEVICE)
criterion = CrossEntropyLoss(weight=weight)
```

### Q3: ì˜¤ë²„í”¼íŒ…

```python
# Early stopping ì¶”ê°€
patience = 3
best_val_f1 = 0
patience_counter = 0

for epoch in range(EPOCHS):
    # ... í•™ìŠµ ...
    
    if val_f1 > best_val_f1:
        best_val_f1 = val_f1
        patience_counter = 0
    else:
        patience_counter += 1
    
    if patience_counter >= patience:
        print("Early stopping!")
        break
```

### Q4: ì¶”ë¡  ì‹œê°„ ì´ˆê³¼

```python
# ëª¨ë¸ ì–‘ìí™” (ì„ íƒ)
import torch.quantization

model = torch.quantization.quantize_dynamic(
    model, 
    {torch.nn.Linear}, 
    dtype=torch.qint8
)
```

---

## ğŸ“Š ì˜ˆìƒ ê²°ê³¼

### íŒŒì¸íŠœë‹ ì „ (ë² ì´ìŠ¤ë¼ì¸)
```
Val F1: ~0.92 (FaceForensics++ val set)
Competition F1: 0.5489
```

### íŒŒì¸íŠœë‹ í›„
```
Val F1: ~0.88-0.92 (FaceForensics++ val set)
Competition F1: 0.60~0.65 (ì˜ˆìƒ)
ê°œì„ : +5~10%
```

---

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] FaceForensics++ ì‹ ì²­ (1~3ì¼)
- [ ] ë°ì´í„° ë‹¤ìš´ë¡œë“œ (1~3ì‹œê°„)
- [ ] ì „ì²˜ë¦¬ ì‹¤í–‰ (2~4ì‹œê°„)
- [ ] í•™ìŠµ ì½”ë“œ ì‘ì„± (1ì‹œê°„)
- [ ] íŒŒì¸íŠœë‹ ì‹¤í–‰ (4~8ì‹œê°„)
- [ ] ë¡œì»¬ ê²€ì¦ (30ë¶„)
- [ ] ëª¨ë¸ ì œì¶œ ì¤€ë¹„ (30ë¶„)
- [ ] ëŒ€íšŒ ì œì¶œ (2~3ì‹œê°„)

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

íŒŒì¸íŠœë‹ ì„±ê³µ í›„:
1. TTA + íŒŒì¸íŠœë‹ ì¡°í•©
2. ì•™ìƒë¸” (ë² ì´ìŠ¤ë¼ì¸ + íŒŒì¸íŠœë‹)
3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
4. ì§ì ‘ ìƒì„± ë°ì´í„° ì¶”ê°€

---

**ì‘ì„±ì¼**: 2025.10.29  
**ë²„ì „**: 1.0  
**ë‚œì´ë„**: ì¤‘ê¸‰  
**ì˜ˆìƒ ì†Œìš”**: 3~5ì¼

