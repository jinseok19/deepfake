{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DeepFake Detection - Fine-tuning with FaceForensics++\n",
        "## EXP-004: FaceForensics++ 데이터셋으로 ViT 모델 파인튜닝\n",
        "\n",
        "### ⚠️ 사전 준비:\n",
        "1. **베이스라인 모델을 Drive에 업로드** (선택사항)\n",
        "   - 로컬: `baseline/model/deep-fake-detector-v2-model/`\n",
        "   - Drive: `/content/drive/MyDrive/deepfake_models/deep-fake-detector-v2-model/`\n",
        "   - 업로드 안 하면 자동으로 원본 ViT 모델 사용\n",
        "\n",
        "2. **FaceForensics++ 데이터 다운로드** (`download_faceforensics.ipynb` 실행)\n",
        "   - Drive: `/content/drive/MyDrive/FaceForensics++/`\n",
        "\n",
        "### 실행 순서:\n",
        "1. 런타임 → 런타임 유형 변경 → **L4 GPU** 선택 ⭐\n",
        "2. 셀 순서대로 실행\n",
        "3. 최종 모델 다운로드 → `submit/models/`에 배치\n",
        "\n",
        "### 예상 시간:\n",
        "- 전처리 (얼굴 추출): 1-2시간\n",
        "- 파인튜닝: 2-3시간\n",
        "- **총 3-5시간**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. 환경 설정\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU 확인\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 필수 라이브러리 설치\n",
        "%pip install -q transformers==4.30.0 torch torchvision pillow opencv-python dlib-bin tqdm scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import dlib\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Google Drive 마운트\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 작업 디렉토리 생성\n",
        "WORK_DIR = \"/content/deepfake_finetune\"\n",
        "DATA_DIR = f\"{WORK_DIR}/data\"\n",
        "OUTPUT_DIR = f\"{WORK_DIR}/output\"\n",
        "DRIVE_SAVE_DIR = \"/content/drive/MyDrive/deepfake_models\"\n",
        "\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(DRIVE_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Working Directory: {WORK_DIR}\")\n",
        "print(f\"Model Save Directory: {DRIVE_SAVE_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. 데이터 준비\n",
        "\n",
        "📦 **FaceForensics++ 사용 (추천)**\n",
        "- `download_faceforensics.ipynb`로 다운로드 완료했다면\n",
        "- Drive 경로만 확인하면 됨!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FaceForensics++ Drive 경로 확인\n",
        "FF_DATA_PATH = \"/content/drive/MyDrive/FaceForensics++\"\n",
        "\n",
        "if os.path.exists(FF_DATA_PATH):\n",
        "    print(\"✅ FaceForensics++ 데이터 발견!\")\n",
        "    print(f\"   경로: {FF_DATA_PATH}\")\n",
        "    \n",
        "    # 데이터 구조 확인\n",
        "    !ls -lh {FF_DATA_PATH}\n",
        "else:\n",
        "    print(\"⚠️ FaceForensics++ 데이터가 없습니다!\")\n",
        "    print(\"   download_faceforensics.ipynb를 먼저 실행하세요\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. 얼굴 추출 전처리\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dlib 얼굴 탐지기 초기화\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "def extract_frames_from_video(video_path, num_frames=30):\n",
        "    \"\"\"비디오에서 균등하게 프레임 샘플링\"\"\"\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    \n",
        "    if total_frames == 0:\n",
        "        cap.release()\n",
        "        return []\n",
        "    \n",
        "    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "    frames = []\n",
        "    \n",
        "    for idx in indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "    \n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "def detect_and_crop_face(image, margin=0.4, target_size=(224, 224)):\n",
        "    \"\"\"얼굴 탐지 및 크롭\"\"\"\n",
        "    if isinstance(image, Image.Image):\n",
        "        image = np.array(image)\n",
        "    \n",
        "    dets = detector(image, 1)\n",
        "    \n",
        "    if len(dets) == 0:\n",
        "        return None\n",
        "    \n",
        "    d = max(dets, key=lambda x: (x.right() - x.left()) * (x.bottom() - x.top()))\n",
        "    \n",
        "    x1, y1, x2, y2 = d.left(), d.top(), d.right(), d.bottom()\n",
        "    w, h = x2 - x1, y2 - y1\n",
        "    \n",
        "    x1 = max(0, int(x1 - w * margin))\n",
        "    y1 = max(0, int(y1 - h * margin))\n",
        "    x2 = min(image.shape[1], int(x2 + w * margin))\n",
        "    y2 = min(image.shape[0], int(y2 + h * margin))\n",
        "    \n",
        "    face = image[y1:y2, x1:x2]\n",
        "    face_pil = Image.fromarray(face).resize(target_size)\n",
        "    \n",
        "    return face_pil\n",
        "\n",
        "print(\"✅ 얼굴 추출 함수 정의 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 전처리 설정\n",
        "NUM_FRAMES = 30\n",
        "PROCESSED_DATA_DIR = f\"{WORK_DIR}/processed_faces\"\n",
        "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
        "os.makedirs(f\"{PROCESSED_DATA_DIR}/real\", exist_ok=True)\n",
        "os.makedirs(f\"{PROCESSED_DATA_DIR}/fake\", exist_ok=True)\n",
        "\n",
        "# FaceForensics++ 비디오 경로 설정\n",
        "FF_DATA_PATH = \"/content/drive/MyDrive/FaceForensics++\"\n",
        "VIDEO_DIRS = {\n",
        "    'real': f\"{FF_DATA_PATH}/original_sequences/youtube/c40/videos\",\n",
        "    'fake': f\"{FF_DATA_PATH}/manipulated_sequences/Deepfakes/c40/videos\"\n",
        "}\n",
        "\n",
        "print(\"📂 비디오 경로:\")\n",
        "print(f\"  Real: {VIDEO_DIRS['real']}\")\n",
        "print(f\"  Fake: {VIDEO_DIRS['fake']}\")\n",
        "\n",
        "# 비디오 파일 수집\n",
        "video_files = {'real': [], 'fake': []}\n",
        "for label, video_dir in VIDEO_DIRS.items():\n",
        "    if os.path.exists(video_dir):\n",
        "        files = list(Path(video_dir).glob('*.mp4'))\n",
        "        video_files[label] = files\n",
        "        print(f\"\\n✅ {label.upper()}: {len(files)} videos found\")\n",
        "    else:\n",
        "        print(f\"\\n⚠️ {video_dir} not found!\")\n",
        "        print(f\"   download_faceforensics.ipynb를 먼저 실행하세요\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 얼굴 추출 실행 (1~2시간 소요)\n",
        "def preprocess_videos(video_files, label):\n",
        "    \"\"\"비디오에서 얼굴 추출 및 저장\"\"\"\n",
        "    face_count = 0\n",
        "    \n",
        "    for video_path in tqdm(video_files, desc=f\"Processing {label}\"):\n",
        "        video_name = video_path.stem\n",
        "        \n",
        "        frames = extract_frames_from_video(video_path, NUM_FRAMES)\n",
        "        \n",
        "        for i, frame in enumerate(frames):\n",
        "            face = detect_and_crop_face(frame)\n",
        "            if face is not None:\n",
        "                save_path = f\"{PROCESSED_DATA_DIR}/{label}/{video_name}_frame{i:03d}.jpg\"\n",
        "                face.save(save_path)\n",
        "                face_count += 1\n",
        "    \n",
        "    return face_count\n",
        "\n",
        "# Real/Fake 비디오 처리\n",
        "for label in ['real', 'fake']:\n",
        "    if len(video_files[label]) > 0:\n",
        "        count = preprocess_videos(video_files[label], label)\n",
        "        print(f\"✅ {label.upper()} faces extracted: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. 데이터셋 준비\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터 리스트 생성\n",
        "real_faces = list(Path(f\"{PROCESSED_DATA_DIR}/real\").glob('*.jpg'))\n",
        "fake_faces = list(Path(f\"{PROCESSED_DATA_DIR}/fake\").glob('*.jpg'))\n",
        "\n",
        "print(f\"Real faces: {len(real_faces)}\")\n",
        "print(f\"Fake faces: {len(fake_faces)}\")\n",
        "\n",
        "data_list = []\n",
        "for face_path in real_faces:\n",
        "    data_list.append({'image_path': str(face_path), 'label': 0})\n",
        "\n",
        "for face_path in fake_faces:\n",
        "    data_list.append({'image_path': str(face_path), 'label': 1})\n",
        "\n",
        "# Train/Val 분할 (80/20)\n",
        "train_data, val_data = train_test_split(\n",
        "    data_list, test_size=0.2, random_state=42, \n",
        "    stratify=[d['label'] for d in data_list]\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain: {len(train_data)} (Real: {sum(1 for d in train_data if d['label']==0)}, Fake: {sum(1 for d in train_data if d['label']==1)})\")\n",
        "print(f\"Val: {len(val_data)} (Real: {sum(1 for d in val_data if d['label']==0)}, Fake: {sum(1 for d in val_data if d['label']==1)})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PyTorch Dataset\n",
        "class DeepfakeDataset(Dataset):\n",
        "    def __init__(self, data_list, processor):\n",
        "        self.data_list = data_list\n",
        "        self.processor = processor\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data_list[idx]\n",
        "        image = Image.open(item['image_path']).convert('RGB')\n",
        "        \n",
        "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "        pixel_values = inputs['pixel_values'].squeeze(0)\n",
        "        \n",
        "        return {\n",
        "            'pixel_values': pixel_values,\n",
        "            'labels': torch.tensor(item['label'], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "print(\"✅ Dataset class 정의 완료\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. 모델 로드 및 파인튜닝\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 베이스라인 모델 로드 (Drive에서)\n",
        "# 방법 1: Drive에 업로드한 경우\n",
        "MODEL_PATH = \"/content/drive/MyDrive/deepfake_models/deep-fake-detector-v2-model\"\n",
        "\n",
        "# 방법 2: 원본 HuggingFace 모델 사용 (인터넷 다운로드)\n",
        "# MODEL_PATH = \"google/vit-base-patch16-224-in21k\"  # 베이스 모델\n",
        "\n",
        "print(f\"모델 로드 중: {MODEL_PATH}\")\n",
        "\n",
        "try:\n",
        "    processor = ViTImageProcessor.from_pretrained(MODEL_PATH)\n",
        "    model = ViTForImageClassification.from_pretrained(MODEL_PATH)\n",
        "    print(f\"✅ Model loaded from Drive!\")\n",
        "except:\n",
        "    print(\"⚠️ Drive에 모델이 없습니다. 원본 ViT 사용...\")\n",
        "    # 베이스라인과 동일한 구조의 원본 모델\n",
        "    MODEL_PATH = \"google/vit-base-patch16-224-in21k\"\n",
        "    processor = ViTImageProcessor.from_pretrained(MODEL_PATH)\n",
        "    model = ViTForImageClassification.from_pretrained(\n",
        "        MODEL_PATH,\n",
        "        num_labels=2,\n",
        "        id2label={0: \"Real\", 1: \"Deepfake\"},\n",
        "        label2id={\"Real\": 0, \"Deepfake\": 1}\n",
        "    )\n",
        "    print(\"✅ Model loaded from HuggingFace Hub (base model)\")\n",
        "\n",
        "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
        "\n",
        "# Dataset 생성\n",
        "train_dataset = DeepfakeDataset(train_data, processor)\n",
        "val_dataset = DeepfakeDataset(val_data, processor)\n",
        "\n",
        "print(f\"✅ Train dataset: {len(train_dataset)}\")\n",
        "print(f\"✅ Val dataset: {len(val_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metric 정의\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    f1_macro = f1_score(labels, predictions, average='macro')\n",
        "    f1_fake = f1_score(labels, predictions, pos_label=1)\n",
        "    \n",
        "    return {\n",
        "        'f1_macro': f1_macro,\n",
        "        'f1_fake': f1_fake\n",
        "    }\n",
        "\n",
        "# Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    fp16=True,\n",
        "    dataloader_num_workers=2,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",  # WandB 비활성화\n",
        ")\n",
        "\n",
        "# Trainer 초기화\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"✅ Trainer 초기화 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 파인튜닝 시작 (2-3시간)\n",
        "print(\"🚀 파인튜닝 시작...\\n\")\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"\\n✅ 파인튜닝 완료!\")\n",
        "print(f\"   최종 Loss: {train_result.training_loss:.4f}\")\n",
        "\n",
        "# Validation 평가\n",
        "eval_result = trainer.evaluate()\n",
        "\n",
        "print(\"\\n📊 Validation 결과:\")\n",
        "for key, value in eval_result.items():\n",
        "    print(f\"   {key}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. 모델 저장 및 다운로드\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 로컬 저장\n",
        "FINAL_MODEL_DIR = f\"{OUTPUT_DIR}/final_model\"\n",
        "trainer.save_model(FINAL_MODEL_DIR)\n",
        "processor.save_pretrained(FINAL_MODEL_DIR)\n",
        "\n",
        "print(f\"✅ 모델 저장: {FINAL_MODEL_DIR}\")\n",
        "\n",
        "# Drive 백업\n",
        "import shutil\n",
        "# FaceForensics++로 학습했지만 이름은 wilddeepfake_finetuned (이미 저장됨)\n",
        "DRIVE_MODEL_PATH = f\"{DRIVE_SAVE_DIR}/wilddeepfake_finetuned\"\n",
        "shutil.copytree(FINAL_MODEL_DIR, DRIVE_MODEL_PATH, dirs_exist_ok=True)\n",
        "\n",
        "print(f\"✅ Drive 백업: {DRIVE_MODEL_PATH}\")\n",
        "print(f\"   (Note: 실제로는 FaceForensics++ 데이터로 학습됨)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ZIP 압축 및 Drive 저장\n",
        "!zip -r {OUTPUT_DIR}/wilddeepfake_model.zip {FINAL_MODEL_DIR}\n",
        "!cp {OUTPUT_DIR}/wilddeepfake_model.zip {DRIVE_SAVE_DIR}/\n",
        "\n",
        "print(f\"✅ ZIP 저장: {DRIVE_SAVE_DIR}/wilddeepfake_model.zip\")\n",
        "print(f\"   (Note: FaceForensics++ 파인튜닝 모델)\")\n",
        "print(f\"   크기: \", end=\"\")\n",
        "!du -sh {DRIVE_SAVE_DIR}/wilddeepfake_model.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. 다음 단계\n",
        "\n",
        "### 로컬 제출 준비:\n",
        "1. Drive에서 `wilddeepfake_model.zip` 다운로드\n",
        "2. 압축 해제 → `submit/models/wilddeepfake_finetuned/` 배치\n",
        "3. `submit/task.ipynb` 수정:\n",
        "```python\n",
        "MODEL_NAME = \"./models/wilddeepfake_finetuned\"\n",
        "```\n",
        "4. 제출:\n",
        "```python\n",
        "aif.submit(model_name=\"EXP-004-WildDeepfake\", key=\"YOUR_KEY\")\n",
        "```\n",
        "\n",
        "### 예상 성능:\n",
        "- Baseline: 0.5489\n",
        "- EXP-002: 0.5506 (+0.31%)\n",
        "- EXP-003: 0.55~0.57 (TTA)\n",
        "- **EXP-004: 0.60~0.65 (파인튜닝)**\n",
        "\n",
        "Val F1이 0.70 이상이면 제출 강력 추천! 🚀\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
