{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DeepFake Detection - Fine-tuning with FaceForensics++\n",
        "## EXP-004: FaceForensics++ ë°ì´í„°ì…‹ìœ¼ë¡œ ViT ëª¨ë¸ íŒŒì¸íŠœë‹\n",
        "\n",
        "### âš ï¸ ì‚¬ì „ ì¤€ë¹„:\n",
        "1. **ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ì„ Driveì— ì—…ë¡œë“œ** (ì„ íƒì‚¬í•­)\n",
        "   - ë¡œì»¬: `baseline/model/deep-fake-detector-v2-model/`\n",
        "   - Drive: `/content/drive/MyDrive/deepfake_models/deep-fake-detector-v2-model/`\n",
        "   - ì—…ë¡œë“œ ì•ˆ í•˜ë©´ ìë™ìœ¼ë¡œ ì›ë³¸ ViT ëª¨ë¸ ì‚¬ìš©\n",
        "\n",
        "2. **FaceForensics++ ë°ì´í„° ë‹¤ìš´ë¡œë“œ** (`download_faceforensics.ipynb` ì‹¤í–‰)\n",
        "   - Drive: `/content/drive/MyDrive/FaceForensics++/`\n",
        "\n",
        "### ì‹¤í–‰ ìˆœì„œ:\n",
        "1. ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ â†’ **L4 GPU** ì„ íƒ â­\n",
        "2. ì…€ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰\n",
        "3. ìµœì¢… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ â†’ `submit/models/`ì— ë°°ì¹˜\n",
        "\n",
        "### ì˜ˆìƒ ì‹œê°„:\n",
        "- ì „ì²˜ë¦¬ (ì–¼êµ´ ì¶”ì¶œ): 1-2ì‹œê°„\n",
        "- íŒŒì¸íŠœë‹: 2-3ì‹œê°„\n",
        "- **ì´ 3-5ì‹œê°„**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. í™˜ê²½ ì„¤ì •\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU í™•ì¸\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "%pip install -q transformers==4.30.0 torch torchvision pillow opencv-python dlib-bin tqdm scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import dlib\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Google Drive ë§ˆìš´íŠ¸\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì‘ì—… ë””ë ‰í† ë¦¬ ìƒì„±\n",
        "WORK_DIR = \"/content/deepfake_finetune\"\n",
        "DATA_DIR = f\"{WORK_DIR}/data\"\n",
        "OUTPUT_DIR = f\"{WORK_DIR}/output\"\n",
        "DRIVE_SAVE_DIR = \"/content/drive/MyDrive/deepfake_models\"\n",
        "\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(DRIVE_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Working Directory: {WORK_DIR}\")\n",
        "print(f\"Model Save Directory: {DRIVE_SAVE_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. ë°ì´í„° ì¤€ë¹„\n",
        "\n",
        "ğŸ“¦ **FaceForensics++ ì‚¬ìš© (ì¶”ì²œ)**\n",
        "- `download_faceforensics.ipynb`ë¡œ ë‹¤ìš´ë¡œë“œ ì™„ë£Œí–ˆë‹¤ë©´\n",
        "- Drive ê²½ë¡œë§Œ í™•ì¸í•˜ë©´ ë¨!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FaceForensics++ Drive ê²½ë¡œ í™•ì¸\n",
        "FF_DATA_PATH = \"/content/drive/MyDrive/FaceForensics++\"\n",
        "\n",
        "if os.path.exists(FF_DATA_PATH):\n",
        "    print(\"âœ… FaceForensics++ ë°ì´í„° ë°œê²¬!\")\n",
        "    print(f\"   ê²½ë¡œ: {FF_DATA_PATH}\")\n",
        "    \n",
        "    # ë°ì´í„° êµ¬ì¡° í™•ì¸\n",
        "    !ls -lh {FF_DATA_PATH}\n",
        "else:\n",
        "    print(\"âš ï¸ FaceForensics++ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n",
        "    print(\"   download_faceforensics.ipynbë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. ì–¼êµ´ ì¶”ì¶œ ì „ì²˜ë¦¬\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dlib ì–¼êµ´ íƒì§€ê¸° ì´ˆê¸°í™”\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "def extract_frames_from_video(video_path, num_frames=30):\n",
        "    \"\"\"ë¹„ë””ì˜¤ì—ì„œ ê· ë“±í•˜ê²Œ í”„ë ˆì„ ìƒ˜í”Œë§\"\"\"\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    \n",
        "    if total_frames == 0:\n",
        "        cap.release()\n",
        "        return []\n",
        "    \n",
        "    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "    frames = []\n",
        "    \n",
        "    for idx in indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "    \n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "def detect_and_crop_face(image, margin=0.4, target_size=(224, 224)):\n",
        "    \"\"\"ì–¼êµ´ íƒì§€ ë° í¬ë¡­\"\"\"\n",
        "    if isinstance(image, Image.Image):\n",
        "        image = np.array(image)\n",
        "    \n",
        "    dets = detector(image, 1)\n",
        "    \n",
        "    if len(dets) == 0:\n",
        "        return None\n",
        "    \n",
        "    d = max(dets, key=lambda x: (x.right() - x.left()) * (x.bottom() - x.top()))\n",
        "    \n",
        "    x1, y1, x2, y2 = d.left(), d.top(), d.right(), d.bottom()\n",
        "    w, h = x2 - x1, y2 - y1\n",
        "    \n",
        "    x1 = max(0, int(x1 - w * margin))\n",
        "    y1 = max(0, int(y1 - h * margin))\n",
        "    x2 = min(image.shape[1], int(x2 + w * margin))\n",
        "    y2 = min(image.shape[0], int(y2 + h * margin))\n",
        "    \n",
        "    face = image[y1:y2, x1:x2]\n",
        "    face_pil = Image.fromarray(face).resize(target_size)\n",
        "    \n",
        "    return face_pil\n",
        "\n",
        "print(\"âœ… ì–¼êµ´ ì¶”ì¶œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì „ì²˜ë¦¬ ì„¤ì •\n",
        "NUM_FRAMES = 30\n",
        "PROCESSED_DATA_DIR = f\"{WORK_DIR}/processed_faces\"\n",
        "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
        "os.makedirs(f\"{PROCESSED_DATA_DIR}/real\", exist_ok=True)\n",
        "os.makedirs(f\"{PROCESSED_DATA_DIR}/fake\", exist_ok=True)\n",
        "\n",
        "# FaceForensics++ ë¹„ë””ì˜¤ ê²½ë¡œ ì„¤ì •\n",
        "FF_DATA_PATH = \"/content/drive/MyDrive/FaceForensics++\"\n",
        "VIDEO_DIRS = {\n",
        "    'real': f\"{FF_DATA_PATH}/original_sequences/youtube/c40/videos\",\n",
        "    'fake': f\"{FF_DATA_PATH}/manipulated_sequences/Deepfakes/c40/videos\"\n",
        "}\n",
        "\n",
        "print(\"ğŸ“‚ ë¹„ë””ì˜¤ ê²½ë¡œ:\")\n",
        "print(f\"  Real: {VIDEO_DIRS['real']}\")\n",
        "print(f\"  Fake: {VIDEO_DIRS['fake']}\")\n",
        "\n",
        "# ë¹„ë””ì˜¤ íŒŒì¼ ìˆ˜ì§‘\n",
        "video_files = {'real': [], 'fake': []}\n",
        "for label, video_dir in VIDEO_DIRS.items():\n",
        "    if os.path.exists(video_dir):\n",
        "        files = list(Path(video_dir).glob('*.mp4'))\n",
        "        video_files[label] = files\n",
        "        print(f\"\\nâœ… {label.upper()}: {len(files)} videos found\")\n",
        "    else:\n",
        "        print(f\"\\nâš ï¸ {video_dir} not found!\")\n",
        "        print(f\"   download_faceforensics.ipynbë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì–¼êµ´ ì¶”ì¶œ ì‹¤í–‰ (1~2ì‹œê°„ ì†Œìš”)\n",
        "def preprocess_videos(video_files, label):\n",
        "    \"\"\"ë¹„ë””ì˜¤ì—ì„œ ì–¼êµ´ ì¶”ì¶œ ë° ì €ì¥\"\"\"\n",
        "    face_count = 0\n",
        "    \n",
        "    for video_path in tqdm(video_files, desc=f\"Processing {label}\"):\n",
        "        video_name = video_path.stem\n",
        "        \n",
        "        frames = extract_frames_from_video(video_path, NUM_FRAMES)\n",
        "        \n",
        "        for i, frame in enumerate(frames):\n",
        "            face = detect_and_crop_face(frame)\n",
        "            if face is not None:\n",
        "                save_path = f\"{PROCESSED_DATA_DIR}/{label}/{video_name}_frame{i:03d}.jpg\"\n",
        "                face.save(save_path)\n",
        "                face_count += 1\n",
        "    \n",
        "    return face_count\n",
        "\n",
        "# Real/Fake ë¹„ë””ì˜¤ ì²˜ë¦¬\n",
        "for label in ['real', 'fake']:\n",
        "    if len(video_files[label]) > 0:\n",
        "        count = preprocess_videos(video_files[label], label)\n",
        "        print(f\"âœ… {label.upper()} faces extracted: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. ë°ì´í„°ì…‹ ì¤€ë¹„\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
        "real_faces = list(Path(f\"{PROCESSED_DATA_DIR}/real\").glob('*.jpg'))\n",
        "fake_faces = list(Path(f\"{PROCESSED_DATA_DIR}/fake\").glob('*.jpg'))\n",
        "\n",
        "print(f\"Real faces: {len(real_faces)}\")\n",
        "print(f\"Fake faces: {len(fake_faces)}\")\n",
        "\n",
        "data_list = []\n",
        "for face_path in real_faces:\n",
        "    data_list.append({'image_path': str(face_path), 'label': 0})\n",
        "\n",
        "for face_path in fake_faces:\n",
        "    data_list.append({'image_path': str(face_path), 'label': 1})\n",
        "\n",
        "# Train/Val ë¶„í•  (80/20)\n",
        "train_data, val_data = train_test_split(\n",
        "    data_list, test_size=0.2, random_state=42, \n",
        "    stratify=[d['label'] for d in data_list]\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain: {len(train_data)} (Real: {sum(1 for d in train_data if d['label']==0)}, Fake: {sum(1 for d in train_data if d['label']==1)})\")\n",
        "print(f\"Val: {len(val_data)} (Real: {sum(1 for d in val_data if d['label']==0)}, Fake: {sum(1 for d in val_data if d['label']==1)})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PyTorch Dataset\n",
        "class DeepfakeDataset(Dataset):\n",
        "    def __init__(self, data_list, processor):\n",
        "        self.data_list = data_list\n",
        "        self.processor = processor\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data_list[idx]\n",
        "        image = Image.open(item['image_path']).convert('RGB')\n",
        "        \n",
        "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "        pixel_values = inputs['pixel_values'].squeeze(0)\n",
        "        \n",
        "        return {\n",
        "            'pixel_values': pixel_values,\n",
        "            'labels': torch.tensor(item['label'], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "print(\"âœ… Dataset class ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. ëª¨ë¸ ë¡œë“œ ë° íŒŒì¸íŠœë‹\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ ë¡œë“œ (Driveì—ì„œ)\n",
        "# ë°©ë²• 1: Driveì— ì—…ë¡œë“œí•œ ê²½ìš°\n",
        "MODEL_PATH = \"/content/drive/MyDrive/deepfake_models/deep-fake-detector-v2-model\"\n",
        "\n",
        "# ë°©ë²• 2: ì›ë³¸ HuggingFace ëª¨ë¸ ì‚¬ìš© (ì¸í„°ë„· ë‹¤ìš´ë¡œë“œ)\n",
        "# MODEL_PATH = \"google/vit-base-patch16-224-in21k\"  # ë² ì´ìŠ¤ ëª¨ë¸\n",
        "\n",
        "print(f\"ëª¨ë¸ ë¡œë“œ ì¤‘: {MODEL_PATH}\")\n",
        "\n",
        "try:\n",
        "    processor = ViTImageProcessor.from_pretrained(MODEL_PATH)\n",
        "    model = ViTForImageClassification.from_pretrained(MODEL_PATH)\n",
        "    print(f\"âœ… Model loaded from Drive!\")\n",
        "except:\n",
        "    print(\"âš ï¸ Driveì— ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ ViT ì‚¬ìš©...\")\n",
        "    # ë² ì´ìŠ¤ë¼ì¸ê³¼ ë™ì¼í•œ êµ¬ì¡°ì˜ ì›ë³¸ ëª¨ë¸\n",
        "    MODEL_PATH = \"google/vit-base-patch16-224-in21k\"\n",
        "    processor = ViTImageProcessor.from_pretrained(MODEL_PATH)\n",
        "    model = ViTForImageClassification.from_pretrained(\n",
        "        MODEL_PATH,\n",
        "        num_labels=2,\n",
        "        id2label={0: \"Real\", 1: \"Deepfake\"},\n",
        "        label2id={\"Real\": 0, \"Deepfake\": 1}\n",
        "    )\n",
        "    print(\"âœ… Model loaded from HuggingFace Hub (base model)\")\n",
        "\n",
        "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
        "\n",
        "# Dataset ìƒì„±\n",
        "train_dataset = DeepfakeDataset(train_data, processor)\n",
        "val_dataset = DeepfakeDataset(val_data, processor)\n",
        "\n",
        "print(f\"âœ… Train dataset: {len(train_dataset)}\")\n",
        "print(f\"âœ… Val dataset: {len(val_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metric ì •ì˜\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    f1_macro = f1_score(labels, predictions, average='macro')\n",
        "    f1_fake = f1_score(labels, predictions, pos_label=1)\n",
        "    \n",
        "    return {\n",
        "        'f1_macro': f1_macro,\n",
        "        'f1_fake': f1_fake\n",
        "    }\n",
        "\n",
        "# Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    fp16=True,\n",
        "    dataloader_num_workers=2,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",  # WandB ë¹„í™œì„±í™”\n",
        ")\n",
        "\n",
        "# Trainer ì´ˆê¸°í™”\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"âœ… Trainer ì´ˆê¸°í™” ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# íŒŒì¸íŠœë‹ ì‹œì‘ (2-3ì‹œê°„)\n",
        "print(\"ğŸš€ íŒŒì¸íŠœë‹ ì‹œì‘...\\n\")\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"\\nâœ… íŒŒì¸íŠœë‹ ì™„ë£Œ!\")\n",
        "print(f\"   ìµœì¢… Loss: {train_result.training_loss:.4f}\")\n",
        "\n",
        "# Validation í‰ê°€\n",
        "eval_result = trainer.evaluate()\n",
        "\n",
        "print(\"\\nğŸ“Š Validation ê²°ê³¼:\")\n",
        "for key, value in eval_result.items():\n",
        "    print(f\"   {key}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. ëª¨ë¸ ì €ì¥ ë° ë‹¤ìš´ë¡œë“œ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë¡œì»¬ ì €ì¥\n",
        "FINAL_MODEL_DIR = f\"{OUTPUT_DIR}/final_model\"\n",
        "trainer.save_model(FINAL_MODEL_DIR)\n",
        "processor.save_pretrained(FINAL_MODEL_DIR)\n",
        "\n",
        "print(f\"âœ… ëª¨ë¸ ì €ì¥: {FINAL_MODEL_DIR}\")\n",
        "\n",
        "# Drive ë°±ì—…\n",
        "import shutil\n",
        "# FaceForensics++ë¡œ í•™ìŠµí–ˆì§€ë§Œ ì´ë¦„ì€ wilddeepfake_finetuned (ì´ë¯¸ ì €ì¥ë¨)\n",
        "DRIVE_MODEL_PATH = f\"{DRIVE_SAVE_DIR}/wilddeepfake_finetuned\"\n",
        "shutil.copytree(FINAL_MODEL_DIR, DRIVE_MODEL_PATH, dirs_exist_ok=True)\n",
        "\n",
        "print(f\"âœ… Drive ë°±ì—…: {DRIVE_MODEL_PATH}\")\n",
        "print(f\"   (Note: ì‹¤ì œë¡œëŠ” FaceForensics++ ë°ì´í„°ë¡œ í•™ìŠµë¨)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ZIP ì••ì¶• ë° Drive ì €ì¥\n",
        "!zip -r {OUTPUT_DIR}/wilddeepfake_model.zip {FINAL_MODEL_DIR}\n",
        "!cp {OUTPUT_DIR}/wilddeepfake_model.zip {DRIVE_SAVE_DIR}/\n",
        "\n",
        "print(f\"âœ… ZIP ì €ì¥: {DRIVE_SAVE_DIR}/wilddeepfake_model.zip\")\n",
        "print(f\"   (Note: FaceForensics++ íŒŒì¸íŠœë‹ ëª¨ë¸)\")\n",
        "print(f\"   í¬ê¸°: \", end=\"\")\n",
        "!du -sh {DRIVE_SAVE_DIR}/wilddeepfake_model.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. ë‹¤ìŒ ë‹¨ê³„\n",
        "\n",
        "### ë¡œì»¬ ì œì¶œ ì¤€ë¹„:\n",
        "1. Driveì—ì„œ `wilddeepfake_model.zip` ë‹¤ìš´ë¡œë“œ\n",
        "2. ì••ì¶• í•´ì œ â†’ `submit/models/wilddeepfake_finetuned/` ë°°ì¹˜\n",
        "3. `submit/task.ipynb` ìˆ˜ì •:\n",
        "```python\n",
        "MODEL_NAME = \"./models/wilddeepfake_finetuned\"\n",
        "```\n",
        "4. ì œì¶œ:\n",
        "```python\n",
        "aif.submit(model_name=\"EXP-004-WildDeepfake\", key=\"YOUR_KEY\")\n",
        "```\n",
        "\n",
        "### ì˜ˆìƒ ì„±ëŠ¥:\n",
        "- Baseline: 0.5489\n",
        "- EXP-002: 0.5506 (+0.31%)\n",
        "- EXP-003: 0.55~0.57 (TTA)\n",
        "- **EXP-004: 0.60~0.65 (íŒŒì¸íŠœë‹)**\n",
        "\n",
        "Val F1ì´ 0.70 ì´ìƒì´ë©´ ì œì¶œ ê°•ë ¥ ì¶”ì²œ! ğŸš€\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
